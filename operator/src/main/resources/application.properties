agent.status.interval=60s
strimzi.bundle.interval=60s
strimzi.bundle.approval-delay=120s
%test.strimzi.bundle.approval-delay=0s

quarkus.log.console.format=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] (%t) %x %s%e%n
# for quarkus 1.x compatibility
quarkus.kubernetes.ports.http.host-port=8080

image.admin-api=quay.io/mk-ci-cd/kafka-admin-api:0.7.0
image.canary=quay.io/mk-ci-cd/strimzi-canary:0.2.0-220111183833
image.canary-init=quay.io/mk-ci-cd/strimzi-canary:0.2.0-220111183833

# unique label required to identify the DrainCleaner's validating webhook
drain.cleaner.webhook.label.key=olm.webhook-description-generate-name
drain.cleaner.webhook.label.value=strimzi-drain-cleaner.kb.io

%dev.quarkus.log.console.level=DEBUG
%dev.quarkus.log.category."org.bf2".level=DEBUG

%test.quarkus.log.category."org.bf2".level=DEBUG

mock.factory.interval=15s

#ingress controller resources - an alternative profile can create fewer/smaller
ingresscontroller.request-cpu=1700m
ingresscontroller.request-memory=1.2Gi
#ingresscontroller.default-replica-count=1
#ingresscontroller.az-replica-count=0

# depends upon the instance type.  It is roughly 300Mi for m5.2xlarge,
ingresscontroller.max-ingress-throughput=300Mi
# percentage of peak throughput you actually need to meet
ingresscontroller.peak-throughput-percentage=40
# this has yet to be completely verified, but is inferred from the
# old sizing - this should go higher, but we should need more memory
ingresscontroller.max-ingress-connections=75000

# external configuration injection through configmap
quarkus.kubernetes-config.enabled=true
quarkus.kubernetes-config.fail-on-missing-config=false
quarkus.kubernetes-config.config-maps=kas-fleetshard-config

# Kubernetes manifest generation
quarkus.kubernetes.service-account=kas-fleetshard-operator
quarkus.kubernetes.labels.app=kas-fleetshard-operator

# deactivate CRD checking from Java Operator SDK
quarkus.operator-sdk.crd.validate=false

# Common prefix for private/internal RHOSAK/Red Hat topics, consumer groups, etc.
managedkafka.kafka.acl.private-prefix=__redhat_

managedkafka.canary.topic=${managedkafka.kafka.acl.private-prefix}strimzi_canary
managedkafka.canary.consumer-group-id=${managedkafka.kafka.acl.private-prefix}strimzi_canary_group
managedkafka.canary.client-id=${managedkafka.kafka.acl.private-prefix}strimzi_canary_client

# Default max session lifetime to 4m 59s
managedkafka.kafka.maximum-session-lifetime-default=299000

# Default canary to probing the external route
managedkafka.canary.probe-external-bootstrap-server-host=true

managedkafka.canary.producer-latency-buckets=50,100,150,200,250,300,350,400,450,500
managedkafka.canary.endtoend-latency-buckets=100,200,300,400,500,600,700,800,900,1000,1100,1200
managedkafka.canary.connection-check-latency-buckets=100,200,300,400,500,600,700,800,900,1000,1100,1200
managedkafka.canary.status-time-window-ms=300000

managedkafka.upgrade.consuming-percentage-threshold=90

# Static ACL static configuration for CustomAuthorizer
managedkafka.kafka.acl.authorizer-class=io.bf2.kafka.authorizer.CustomAclAuthorizer
managedkafka.kafka.acl.config-prefix=strimzi.authorization.custom-authorizer.
managedkafka.kafka.acl.allowed-listeners=SRE-9096
managedkafka.kafka.acl.logging.suppression-window.duration=PT300S
managedkafka.kafka.acl.logging.suppression-window.eventCount=5000
managedkafka.kafka.acl.logging.suppression-window.apis=PRODUCE,FETCH

# Global + Default ACLs
# - Default clusters to allow describe of all topics, consumer groups, and ACLs
# - Globally deny cluster operations other than idempotent_write, describe_acls, create_acls, and delete_acls
managedkafka.kafka.acl.global=\
default=true;permission=allow;topic=*;operations=describe,describe_configs \n\
default=true;permission=allow;group=*;operations=describe \n\
default=true;permission=allow;cluster=*;operations=describe \n\
permission=deny;cluster=*;operations-except=alter,describe,idempotent_write \n\
permission=deny;cluster=*;operations=alter;apis-except=create_acls,delete_acls \n\
permission=deny;cluster=*;operations=describe;apis-except=describe_acls \n\
permission=allow;cluster=*;operations=idempotent_write \n\
priority=1;permission=deny;topic=__consumer_offsets;operations=all \n\
priority=1;permission=deny;topic=__transaction_state;operations=all \n\
priority=1;permission=deny;topic=${managedkafka.kafka.acl.private-prefix}*;operations=all \n\
priority=1;permission=deny;group=${managedkafka.kafka.acl.private-prefix}*;operations=all \n\
priority=1;permission=deny;transactional_id=${managedkafka.kafka.acl.private-prefix}*;operations=all

# Allow "owner" users full control of topics, groups, transactional_ids, and cluster ACLs
managedkafka.kafka.acl.owner=\
priority=1;permission=allow;principal=%1$s;cluster=*;operations=describe;apis=describe_acls \n\
priority=1;permission=allow;principal=%1$s;cluster=*;operations=alter;apis=create_acls,delete_acls \n\
priority=1;permission=allow;principal=%1$s;topic=*;operations=all \n\
priority=1;permission=allow;principal=%1$s;group=*;operations=all \n\
priority=1;permission=allow;principal=%1$s;transactional_id=*;operations=all

# "canary" service account allowed to read/write own topic, read own consumer group
managedkafka.kafka.acl.service-accounts.canary=\
priority=1;permission=allow;principal=%1$s;cluster=*;operations=describe;apis=list_partition_reassignments \n\
priority=1;permission=allow;principal=%1$s;cluster=*;operations=alter;apis=alter_partition_reassignments \n\
priority=0;permission=allow;principal=%1$s;topic=${managedkafka.canary.topic};operations=create,describe,read,write,alter,alter_configs \n\
priority=0;permission=allow;principal=%1$s;group=${managedkafka.canary.consumer-group-id};operations=describe,read \n\
priority=1;permission=deny;principal=%1$s;topic=*;operations=all \n\
priority=1;permission=deny;principal=%1$s;group=*;operations=all \n\
priority=1;permission=deny;principal=%1$s;transactional_id=*;operations=all

# Used for validation in Admin API and custom Kafka Authorizer
managedkafka.kafka.acl.resource-operations={ "cluster": [ "describe", "alter" ], "group": [ "all", "delete", "describe", "read" ], "topic": [ "all", "alter", "alter_configs", "create", "delete", "describe", "describe_configs", "read", "write" ], "transactional_id": [ "all", "describe", "write" ] }

# ACL logging config
# Used to configure per-action log levels
managedkafka.kafka.acl.logging=\
cluster=*;apis=fetch,list_groups,describe_configs;level=DEBUG \n\
topic=*;apis=list_offsets;level=DEBUG \n\
topic=*;operations=describe;level=DEBUG \n\
priority=1;topic=${managedkafka.canary.topic};operations=describe,read,write;level=DEBUG \n\
group=*;apis=offset_fetch,offset_commit,heartbeat,describe_groups;level=DEBUG

managedkafka.zookeeper.volume-size=10Gi

# default values for spec capacity - should be removed or replaced with a hard capacity
managedkafka.kafka.max-connections=3000
managedkafka.kafka.message-max-bytes=1048588
managedkafka.kafka.connection-attempts-per-sec=100
managedkafka.kafka.ingress-per-sec=30Mi
managedkafka.kafka.egress-per-sec=30Mi

# topology settings
managedkafka.kafka.one-instance-per-node=false
managedkafka.kafka.colocate-with-zookeeper=true
managedkafka.adminserver.colocate-with-zookeeper=true
managedkafka.kafkaexporter.colocate-with-zookeeper=true
managedkafka.canary.colocate-with-zookeeper=true
# if this is overriden, the properties below need overriden
managedkafka.kafka.scaling-and-replication-factor=3
managedkafka.zookeeper.replicas=3

# other default ga / su pod sizings

managedkafka.kafkaexporter.container-cpu=1000m
managedkafka.kafkaexporter.container-request-cpu=500m
managedkafka.kafkaexporter.container-memory=256Mi

managedkafka.canary.container-cpu=75m
managedkafka.canary.container-memory=64Mi

managedkafka.adminserver.container-cpu=500m
managedkafka.adminserver.container-memory=512Mi

managedkafka.zookeeper.jvm-xms=1G
managedkafka.zookeeper.container-cpu=1000m
managedkafka.zookeeper.container-memory=4Gi

# sizing settings specific to OpenShift 4.8 on m5.2xlarge, and the topology settings
managedkafka.kafka.container-cpu=4500m
managedkafka.kafka.container-memory=19Gi
managedkafka.kafka.jvm-xms=6442450944

# absolute capacity specific to the above

# this is meant as a pre-replication per broker value
#
# the spec capacity value is understood to be
# X fully replicated partitions typically with replication factor 3
managedkafka.kafka.partition-capacity=1500

quarkus.arc.test.disable-application-lifecycle-observers=true

managedkafka.canary.init-enabled=true
managedkafka.canary.init-timeout-seconds=600

# Storage settings for quota plugin
managedkafka.storage.check-interval=30
managedkafka.storage.safety-factor=2
managedkafka.storage.min-margin=10Gi
